"""
LangChain ê¸°ë°˜ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ AI í”„ë¡œë°”ì´ë”
EduGPTì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” LangChain í†µí•© ë²„ì „
"""

import os
import logging
from typing import Optional, Dict, Any, Union, List
from enum import Enum
from dataclasses import dataclass

# LangChain ì„í¬íŠ¸
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel

from app.core.config import settings

logger = logging.getLogger(__name__)

class EduGPTProvider(Enum):
    """EduGPTì—ì„œ ì‚¬ìš©í•  AI ì œê³µì"""
    OPENAI_DIRECT = "openai_direct"      # OpenAI API ì§ì ‘ ì‚¬ìš©
    OPENROUTER_FALLBACK = "openrouter"   # OpenRouter í´ë°±

@dataclass
class EduGPTConfig:
    """EduGPT AI ì„¤ì •"""
    provider: EduGPTProvider
    model_name: str
    api_key: str
    base_url: str
    temperature: float = 0.7
    max_tokens: int = 4096

class LangChainHybridProvider:
    """
    LangChain ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ AI ì œê³µì
    EduGPTì˜ DiscussAgentì™€ ì™„ì „ í˜¸í™˜
    """
    
    def __init__(self):
        self.config = self._determine_provider()
        self.chat_model = self._create_chat_model()
        
        # ì†ì„± ì¶”ê°€ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±)
        self.current_provider = self.config.provider.value if self.config else "none"
        self.openai_available = self.config and self.config.provider == EduGPTProvider.OPENAI_DIRECT
        self.openrouter_available = self.config and self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK
        
    def _determine_provider(self) -> EduGPTConfig:
        """ì‚¬ìš©í•  AI ì œê³µì ê²°ì •"""
        
        # 1ìˆœìœ„: OpenAI API í‚¤ í™•ì¸
        openai_key = os.getenv("OPENAI_API_KEY") or getattr(settings, 'OPENAI_API_KEY', None)
        
        if openai_key and openai_key.startswith("sk-"):
            logger.info("ğŸ”‘ OpenAI API key detected - using OpenAI for EduGPT")
            return EduGPTConfig(
                provider=EduGPTProvider.OPENAI_DIRECT,
                model_name="gpt-3.5-turbo",
                api_key=openai_key,
                base_url="https://api.openai.com/v1",
                temperature=0.7,
                max_tokens=4096
            )
        
        # 2ìˆœìœ„: OpenRouter API í‚¤ ì‚¬ìš©
        openrouter_key = os.getenv("OPENROUTER_API_KEY") or getattr(settings, 'OPENROUTER_API_KEY', None)
        
        if openrouter_key:
            logger.info("ğŸ”„ No OpenAI key - using OpenRouter for EduGPT (cost-effective)")
            return EduGPTConfig(
                provider=EduGPTProvider.OPENROUTER_FALLBACK,
                model_name="x-ai/grok-4-fast:free",
                api_key=openrouter_key,
                base_url="https://openrouter.ai/api/v1",
                temperature=0.7,
                max_tokens=4096
            )
        
        raise ValueError("âŒ No AI API keys found. Please set OPENAI_API_KEY or OPENROUTER_API_KEY")
    
    def _create_chat_model(self) -> BaseChatModel:
        """LangChain ChatModel ìƒì„±"""
        
        if self.config.provider == EduGPTProvider.OPENAI_DIRECT:
            # OpenAI ì§ì ‘ ì—°ê²°
            return ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self.config.api_key,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
        
        elif self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK:
            # OpenRouterë¥¼ í†µí•œ ì—°ê²° (í—¤ë” ì„¤ì • ìˆ˜ì •)
            return ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self.config.api_key,
                openai_api_base=self.config.base_url,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                default_headers={
                    "HTTP-Referer": "https://lms-mvp.com",
                    "X-Title": "LMS MVP - EduGPT Integration"
                }
            )
        
        raise ValueError(f"Unsupported provider: {self.config.provider}")
    
    def get_llm(self) -> BaseChatModel:
        """ì¼ë°˜ LLM ë°˜í™˜"""
        return self.chat_model
    
    def get_streaming_llm(self, callbacks: List = None) -> BaseChatModel:
        """ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥í•œ LLM ë°˜í™˜"""
        if self.config.provider == EduGPTProvider.OPENAI_DIRECT:
            return ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self.config.api_key,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=True,
                callbacks=callbacks or []
            )
        
        elif self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK:
            return ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self.config.api_key,
                openai_api_base=self.config.base_url,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=True,
                callbacks=callbacks or [],
                default_headers={
                    "HTTP-Referer": "https://lms-mvp.com",
                    "X-Title": "LMS MVP - EduGPT Streaming"
                }
            )
        
        raise ValueError(f"Unsupported provider for streaming: {self.config.provider}")
    
    def invoke_with_messages(self, messages: List[BaseMessage]) -> AIMessage:
        """LangChain ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ AI í˜¸ì¶œ (EduGPT í˜¸í™˜)"""
        try:
            response = self.chat_model.invoke(messages)
            return response
        except Exception as e:
            logger.error(f"âŒ LangChain AI invoke failed: {e}")
            raise
    
    def generate_with_system_and_human(self, system_content: str, human_content: str) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì¸ê°„ ë©”ì‹œì§€ë¡œ ì‘ë‹µ ìƒì„±"""
        
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=human_content)
        ]
        
        response = self.invoke_with_messages(messages)
        return response.content
    
    async def generate_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ ë¹„ë™ê¸° ì‘ë‹µ ìƒì„±"""
        
        # ì¼ì‹œì ìœ¼ë¡œ ì˜¨ë„ ì¡°ì ˆ
        original_temp = self.chat_model.temperature
        self.chat_model.temperature = temperature
        
        try:
            human_message = HumanMessage(content=prompt)
            response = self.chat_model.invoke([human_message])
            return response.content
        finally:
            # ì˜¨ë„ ë³µì›
            self.chat_model.temperature = original_temp
    
    def get_provider_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì œê³µì ì •ë³´"""
        return {
            "provider": self.config.provider.value,
            "model": self.config.model_name,
            "framework": "langchain",
            "is_free": self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK,
            "cost_optimization": True if self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK else False
        }
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ì¶”ì •"""
        
        if self.config.provider == EduGPTProvider.OPENROUTER_FALLBACK:
            if "free" in self.config.model_name:
                return 0.0  # ë¬´ë£Œ ëª¨ë¸
            else:
                return (input_tokens + output_tokens) * 0.001 / 1000  # $0.001 per 1K tokens
        
        elif self.config.provider == EduGPTProvider.OPENAI_DIRECT:
            # OpenAI GPT-3.5-turbo ë¹„ìš©
            input_cost = input_tokens * 0.0015 / 1000   # $0.0015 per 1K input tokens
            output_cost = output_tokens * 0.002 / 1000  # $0.002 per 1K output tokens
            return input_cost + output_cost
        
        return 0.0


class EduGPTDiscussAgent:
    """
    EduGPTì˜ DiscussAgentë¥¼ LangChain ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„
    ì›ë³¸ê³¼ 100% í˜¸í™˜
    """
    
    def __init__(
        self,
        system_message: SystemMessage,
        model: BaseChatModel,
    ) -> None:
        self.system_message = system_message
        self.model = model
        self.init_messages()

    def reset(self) -> List[BaseMessage]:
        """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ë¦¬ì…‹"""
        self.init_messages()
        return self.stored_messages

    def init_messages(self) -> None:
        """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.stored_messages = [self.system_message]

    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:
        """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.stored_messages.append(message)
        return self.stored_messages

    def step(self, input_message: HumanMessage) -> AIMessage:
        """í•œ ë‹¨ê³„ ëŒ€í™” ì§„í–‰ (EduGPT ì›ë³¸ê³¼ ë™ì¼)"""
        messages = self.update_messages(input_message)
        
        output_message = self.model.invoke(messages)
        self.update_messages(output_message)
        
        return output_message


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_langchain_hybrid_provider = None

def get_langchain_hybrid_provider() -> LangChainHybridProvider:
    """LangChain í•˜ì´ë¸Œë¦¬ë“œ ì œê³µì ì‹±ê¸€í†¤"""
    global _langchain_hybrid_provider
    if _langchain_hybrid_provider is None:
        _langchain_hybrid_provider = LangChainHybridProvider()
    return _langchain_hybrid_provider

def create_discuss_agent(system_message_content: str) -> EduGPTDiscussAgent:
    """EduGPT ìŠ¤íƒ€ì¼ DiscussAgent ìƒì„±"""
    provider = get_langchain_hybrid_provider()
    system_message = SystemMessage(content=system_message_content)
    return EduGPTDiscussAgent(system_message, provider.chat_model)
