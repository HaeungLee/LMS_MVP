"""
AI ì œê³µì í†µí•© ì‹œìŠ¤í…œ - Phase 4
- OpenRouter ê¸°ë³¸ ì§€ì› (ë¬´ë£Œ/ì €ë¹„ìš© ëª¨ë¸)
- OpenAI í˜¸í™˜ (í•„ìš”ì‹œ ì‰¬ìš´ ì „í™˜)
- ëª¨ë¸ë³„ íŠ¹ì„± ìµœì í™”
- ë¹„ìš© íš¨ìœ¨ì  ìš´ì˜
"""

import asyncio
import json
import logging
import os
from typing import Dict, Any, List, Optional, Union
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
import openai
from openai import AsyncOpenAI

from app.core.config import settings
from app.services.redis_service import get_redis_service
from app.services.advanced_llm_optimizer import get_llm_optimizer

logger = logging.getLogger(__name__)

class AIProvider(Enum):
    """AI ì œê³µì"""
    OPENROUTER = "openrouter"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

class ModelTier(Enum):
    """ëª¨ë¸ ë“±ê¸‰ (ë¹„ìš© ê¸°ì¤€)"""
    FREE = "free"           # ë¬´ë£Œ ëª¨ë¸
    BASIC = "basic"         # ì €ë¹„ìš© ëª¨ë¸  
    PREMIUM = "premium"     # ê³ ì„±ëŠ¥ ëª¨ë¸
    ENTERPRISE = "enterprise"  # ìµœê³ ê¸‰ ëª¨ë¸

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    name: str
    provider: AIProvider
    tier: ModelTier
    cost_per_1k_tokens: float
    max_tokens: int
    context_window: int
    strengths: List[str]
    best_for: List[str]

@dataclass
class AIRequest:
    """AI ìš”ì²­"""
    prompt: str
    max_tokens: Optional[int] = None
    temperature: float = 0.7
    model_preference: Optional[ModelTier] = None
    task_type: str = "general"
    user_id: Optional[int] = None
    priority: str = "normal"

class AIProviderManager:
    """AI ì œê³µì í†µí•© ê´€ë¦¬"""
    
    def __init__(self):
        self.redis_service = get_redis_service()
        self.llm_optimizer = get_llm_optimizer()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ì˜
        self.models = {
            # OpenRouter ë¬´ë£Œ/ì €ë¹„ìš© ëª¨ë¸ (í˜„ì¬ ì‚¬ìš© ë¶ˆê°€ë¡œ ì£¼ì„ ì²˜ë¦¬)
            # "google/gemma-2-9b-it:free": ModelConfig(
            #     name="google/gemma-2-9b-it:free",
            #     provider=AIProvider.OPENROUTER,
            #     tier=ModelTier.FREE,
            #     cost_per_1k_tokens=0.0,
            #     max_tokens=8192,
            #     context_window=8192,
            #     strengths=["ë¹ ë¥¸ ì‘ë‹µ", "ê¸°ë³¸ ì¶”ë¡ ", "ì½”ë”© ì§€ì›"],
            #     best_for=["ì¼ë°˜ í”¼ë“œë°±", "ê°„ë‹¨í•œ ì§ˆë¬¸ ë‹µë³€", "ê¸°ì´ˆ ë¶„ì„"]
            # ),
            # âœ… ì•ˆì •ì ì¸ ë¬´ë£Œ ëª¨ë¸ë¡œ êµì²´ (Mistral 7B)
            "mistralai/mistral-7b-instruct:free": ModelConfig(
                name="mistralai/mistral-7b-instruct:free",
                provider=AIProvider.OPENROUTER,
                tier=ModelTier.FREE,
                cost_per_1k_tokens=0.0,
                max_tokens=4096,
                context_window=4096,
                strengths=["êµìœ¡ ì½˜í…ì¸ ", "ì¼ë°˜ ëŒ€í™”", "ë¬¸ì œ í•´ê²°", "ì½”ë“œ ì§€ì›"],
                best_for=["í•™ìŠµ ì§€ì›", "ì‹¤ì‹œê°„ í”¼ë“œë°±", "í”„ë¡œê·¸ë˜ë° êµìœ¡"]
            ),

            
            # OpenRouter ì €ë¹„ìš© ëª¨ë¸
            "anthropic/claude-3-haiku": ModelConfig(
                name="anthropic/claude-3-haiku",
                provider=AIProvider.OPENROUTER,
                tier=ModelTier.BASIC,
                cost_per_1k_tokens=0.25,
                max_tokens=4096,
                context_window=200000,
                strengths=["ë¹ ë¥´ê³  ì •í™•", "ì•ˆì „í•œ ì‘ë‹µ", "ë¶„ì„ì "],
                best_for=["ìƒì„¸ í”¼ë“œë°±", "ë©˜í† ë§", "í•™ìŠµ ê³„íš"]
            ),
            "google/gemini-flash-1.5": ModelConfig(
                name="google/gemini-flash-1.5",
                provider=AIProvider.OPENROUTER,
                tier=ModelTier.BASIC,
                cost_per_1k_tokens=0.075,
                max_tokens=8192,
                context_window=1000000,
                strengths=["ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸", "ë©€í‹°ëª¨ë‹¬", "ë¹ ë¥¸ ì²˜ë¦¬"],
                best_for=["í”„ë¡œì íŠ¸ ë¶„ì„", "ì½”ë“œ ë¦¬ë·°", "í¬íŠ¸í´ë¦¬ì˜¤ í‰ê°€"]
            ),
            
            # OpenAI í˜¸í™˜ (í•„ìš”ì‹œ ì‚¬ìš©)
            "gpt-3.5-turbo": ModelConfig(
                name="gpt-3.5-turbo",
                provider=AIProvider.OPENAI,
                tier=ModelTier.BASIC,
                cost_per_1k_tokens=0.5,
                max_tokens=4096,
                context_window=16385,
                strengths=["ë²”ìš©ì„±", "ì•ˆì •ì„±", "ë¹ ë¥¸ ì‘ë‹µ"],
                best_for=["ì¼ë°˜ ëŒ€í™”", "í”¼ë“œë°±", "ì§ˆë¬¸ ë‹µë³€"]
            ),
            "gpt-4o-mini": ModelConfig(
                name="gpt-4o-mini",
                provider=AIProvider.OPENAI,
                tier=ModelTier.PREMIUM,
                cost_per_1k_tokens=0.15,
                max_tokens=16384,
                context_window=128000,
                strengths=["ê³ í’ˆì§ˆ ì¶”ë¡ ", "ì°½ì˜ì„±", "ë³µì¡í•œ ë¶„ì„"],
                best_for=["ì‹¬ì¸µ ë¶„ì„", "ê°œì¸í™” ì¶”ì²œ", "ê³ ê¸‰ ë©˜í† ë§"]
            ),
            "gpt-4o": ModelConfig(
                name="gpt-4o",
                provider=AIProvider.OPENAI,
                tier=ModelTier.ENTERPRISE,
                cost_per_1k_tokens=2.5,
                max_tokens=4096,
                context_window=128000,
                strengths=["ìµœê³  í’ˆì§ˆ", "ë³µì¡í•œ ì¶”ë¡ ", "ë©€í‹°ëª¨ë‹¬"],
                best_for=["ìµœê³ ê¸‰ ë¶„ì„", "ì—°êµ¬", "ì „ë¬¸ê°€ ìˆ˜ì¤€ í”¼ë“œë°±"]
            )
        }
        
        # ğŸš€ ì‘ì—… ìœ í˜•ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘ (ì•ˆì •ì ì¸ Mistralë¡œ ë³€ê²½)
        self.task_model_mapping = {
            "feedback": {
                ModelTier.FREE: "mistralai/mistral-7b-instruct:free",  # ì•ˆì •ì ì¸ êµìœ¡ ëª¨ë¸
                ModelTier.BASIC: "anthropic/claude-3-haiku",
                ModelTier.PREMIUM: "gpt-4o-mini"
            },
            "analysis": {
                ModelTier.FREE: "mistralai/mistral-7b-instruct:free",  # ì•ˆì •ì ì¸ ë¶„ì„ ëª¨ë¸
                ModelTier.BASIC: "google/gemini-flash-1.5",
                ModelTier.PREMIUM: "gpt-4o-mini"
            },
            "coding": {
                ModelTier.FREE: "mistralai/mistral-7b-instruct:free",  # ì½”ë”© ì§€ì› ëª¨ë¸
                ModelTier.BASIC: "anthropic/claude-3-haiku",
                ModelTier.PREMIUM: "gpt-4o-mini"
            },
            "mentoring": {
                ModelTier.FREE: "mistralai/mistral-7b-instruct:free",  # êµìœ¡ìš© ëŒ€í™” ëª¨ë¸
                ModelTier.BASIC: "anthropic/claude-3-haiku",
                ModelTier.PREMIUM: "gpt-4o-mini"
            },
            "project_review": {
                ModelTier.FREE: "mistralai/mistral-7b-instruct:free",  # ì½”ë“œ ë¦¬ë·° ëª¨ë¸
                ModelTier.BASIC: "google/gemini-flash-1.5",
                ModelTier.PREMIUM: "gpt-4o"
            }
        }
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.clients = {}
        self._initialize_clients()
    
    def _initialize_clients(self):
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenRouter í´ë¼ì´ì–¸íŠ¸ (OpenAI í˜¸í™˜)
            if hasattr(settings, 'openrouter_api_key') and settings.openrouter_api_key:
                self.clients[AIProvider.OPENROUTER] = AsyncOpenAI(
                    base_url="https://openrouter.ai/api/v1",
                    api_key=settings.openrouter_api_key,
                )
                logger.info("OpenRouter í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸
            if hasattr(settings, 'openai_api_key') and settings.openai_api_key:
                self.clients[AIProvider.OPENAI] = AsyncOpenAI(
                    api_key=settings.openai_api_key,
                )
                logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def select_optimal_model(self, request: AIRequest) -> ModelConfig:
        """ìµœì  ëª¨ë¸ ì„ íƒ"""
        
        # 1. ì‚¬ìš©ì ì„ í˜¸ë„ ë˜ëŠ” ê¸°ë³¸ ë“±ê¸‰ ê²°ì •
        preferred_tier = request.model_preference or ModelTier.FREE
        
        # 2. ì‘ì—… ìœ í˜•ë³„ ìµœì  ëª¨ë¸ ì°¾ê¸°
        task_models = self.task_model_mapping.get(request.task_type, self.task_model_mapping["feedback"])
        
        # 3. ì„ í˜¸ ë“±ê¸‰ì˜ ëª¨ë¸ ì„ íƒ
        model_name = task_models.get(preferred_tier)
        if not model_name:
            # í´ë°±: FREE ë“±ê¸‰ ëª¨ë¸ ì‚¬ìš©
            model_name = task_models.get(ModelTier.FREE)
        
        # 4. ëª¨ë¸ ì„¤ì • ë°˜í™˜
        model_config = self.models.get(model_name)
        if not model_config:
            # ğŸš€ ìµœì¢… í´ë°±: ì•ˆì •ì ì¸ Mistral 7B ëª¨ë¸
            model_config = self.models["mistralai/mistral-7b-instruct:free"]
        
        logger.info(f"ì„ íƒëœ ëª¨ë¸: {model_config.name} (ë“±ê¸‰: {model_config.tier.value})")
        return model_config
    
    async def generate_completion(self, request: AIRequest) -> Dict[str, Any]:
        """AI ì™„ì„± ìƒì„±"""
        
        try:
            # ìµœì  ëª¨ë¸ ì„ íƒ
            model_config = self.select_optimal_model(request)
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(request.prompt, model_config.name)
            cached_response = self.redis_service.get_llm_cache(cache_key)
            
            if cached_response:
                logger.info(f"ìºì‹œ íˆíŠ¸: {model_config.name}")
                return {
                    'success': True,
                    'response': cached_response,
                    'model': model_config.name,
                    'provider': model_config.provider.value,
                    'tier': model_config.tier.value,
                    'cached': True,
                    'cost_saved': model_config.cost_per_1k_tokens * (len(request.prompt) / 1000)
                }
            
            # AI í˜¸ì¶œ
            response = await self._call_ai_api(request, model_config)
            
            if response['success']:
                # ìºì‹œ ì €ì¥
                # ğŸš€ ìºì‹œ ìµœì í™”: ë©˜í† ë§ì€ ì§§ì€ TTL, ì¼ë°˜ì€ ê¸´ TTL
                if "mentoring" in request.task_type:
                    ttl_seconds = 1800  # ë©˜í† ë§: 30ë¶„ (ëŒ€í™” ë§¥ë½ ìœ ì§€)
                else:
                    ttl_seconds = 14400  # ì¼ë°˜: 4ì‹œê°„

                self.redis_service.set_llm_cache(cache_key, response['response'], ttl_seconds)
                
                # ë¹„ìš© ì¶”ì 
                await self._track_usage(request.user_id, model_config, response.get('tokens_used', 0))
            
            return response
            
        except Exception as e:
            logger.error(f"AI ì™„ì„± ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'fallback_used': True
            }
    
    async def _call_ai_api(self, request: AIRequest, model_config: ModelConfig) -> Dict[str, Any]:
        """ì‹¤ì œ AI API í˜¸ì¶œ"""
        
        client = self.clients.get(model_config.provider)
        if not client:
            raise Exception(f"í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ: {model_config.provider}")
        
        try:
            # í† í° ìˆ˜ ì œí•œ ì ìš©
            max_tokens = min(
                request.max_tokens or model_config.max_tokens,
                model_config.max_tokens
            )
            
            # ğŸš€ ì†ë„ ìµœì í™”ëœ API í˜¸ì¶œ
            completion = await client.chat.completions.create(
                model=model_config.name,
                messages=[
                    {"role": "user", "content": request.prompt}
                ],
                max_tokens=min(max_tokens, 2048),  # ë” ìì„¸í•œ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€ (1024 -> 2048)
                temperature=min(request.temperature or 0.7, 0.9),  # ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
                top_p=0.9,  # ë‹¤ì–‘ì„± ì¦ê°€
                frequency_penalty=0.0,
                presence_penalty=0.0,
                stream=False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”ë¡œ ì´ˆê¸° ì‘ë‹µ ì†ë„ í–¥ìƒ
            )
            
            response_text = completion.choices[0].message.content
            
            # LLM íŠ¹ìˆ˜ í† í° ì œê±°
            if response_text:
                # <s>, </s>, <|im_start|>, <|im_end|> ë“± ëª¨ë“  íŠ¹ìˆ˜ í† í° ì œê±°
                response_text = response_text.replace('<s>', '').replace('</s>', '')
                response_text = response_text.replace('<|im_start|>', '').replace('<|im_end|>', '')
                response_text = response_text.replace('<|endoftext|>', '')
                response_text = response_text.replace('[INST]', '').replace('[/INST]', '')
                response_text = response_text.replace('[B_INST]', '').replace('[/B_INST]', '')
                response_text = response_text.replace('[BOS]', '').replace('[/BOS]', '')
                response_text = response_text.replace('[EOS]', '').replace('[/EOS]', '')
                response_text = response_text.replace('<<SYS>>', '').replace('<</SYS>>', '')
                response_text = response_text.replace('[BOT]', '').replace('[/BOT]', '')
                response_text = response_text.replace('[USER]', '').replace('[/USER]', '')
                response_text = response_text.replace('[ASSISTANT]', '').replace('[/ASSISTANT]', '')
                # ì•ë’¤ ê³µë°± ì œê±°
                response_text = response_text.strip()
            
            tokens_used = completion.usage.total_tokens if completion.usage else 0
            
            return {
                'success': True,
                'response': response_text,
                'model': model_config.name,
                'provider': model_config.provider.value,
                'tier': model_config.tier.value,
                'tokens_used': tokens_used,
                'cost_estimate': (tokens_used / 1000) * model_config.cost_per_1k_tokens,
                'cached': False
            }
            
        except Exception as e:
            logger.error(f"AI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # í´ë°± ì²˜ë¦¬
            if model_config.tier != ModelTier.FREE:
                # ë¬´ë£Œ ëª¨ë¸ë¡œ ì¬ì‹œë„
                free_request = AIRequest(
                    prompt=request.prompt,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                    model_preference=ModelTier.FREE,
                    task_type=request.task_type,
                    user_id=request.user_id
                )
                return await self.generate_completion(free_request)
            
            raise e
    
    def _generate_cache_key(self, prompt: str, model_name: str) -> str:
        """ğŸš€ ê³ ì† ìºì‹œ í‚¤ ìƒì„± (ë©˜í† ë§ìš© ê°œì„ )"""
        import hashlib
        import time

        # ë©˜í† ë§ í”„ë¡¬í”„íŠ¸ì˜ ê²½ìš° íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ë¡œ ìºì‹œ ì¶©ëŒ ë°©ì§€
        if "AI í•™ìŠµ ë©˜í† " in prompt or "ë©˜í† ë§" in prompt:
            timestamp = str(int(time.time()))  # í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            normalized_prompt = prompt.strip().lower()[:300]  # 300ìë¡œ ì¦ê°€
            content = f"{model_name}:{normalized_prompt}:{timestamp}"
        else:
            # ì¼ë°˜ í”„ë¡¬í”„íŠ¸ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
            normalized_prompt = prompt.strip().lower()[:200]
            content = f"{model_name}:{normalized_prompt}"

        # ë” ê¸´ í•´ì‹œ ì‚¬ìš© (ì¶©ëŒ ë°©ì§€)
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    async def _track_usage(self, user_id: Optional[int], model_config: ModelConfig, tokens_used: int):
        """ì‚¬ìš©ëŸ‰ ì¶”ì """
        try:
            usage_data = {
                'user_id': user_id,
                'model': model_config.name,
                'provider': model_config.provider.value,
                'tier': model_config.tier.value,
                'tokens_used': tokens_used,
                'cost': (tokens_used / 1000) * model_config.cost_per_1k_tokens,
                'timestamp': datetime.utcnow().isoformat()
            }
            
            # Redisì— ì‚¬ìš©ëŸ‰ ê¸°ë¡
            usage_key = f"ai_usage:{datetime.utcnow().strftime('%Y-%m-%d')}"
            usage_list = self.redis_service.get_cache(usage_key) or []
            usage_list.append(usage_data)
            
            # ì¼ì¼ ì‚¬ìš©ëŸ‰ ìœ ì§€ (ìµœëŒ€ 1000ê°œ ê¸°ë¡)
            if len(usage_list) > 1000:
                usage_list = usage_list[-1000:]
            
            self.redis_service.set_cache(usage_key, usage_list, 86400)  # 24ì‹œê°„
            
            logger.debug(f"ì‚¬ìš©ëŸ‰ ì¶”ì : {model_config.name} - {tokens_used} í† í°")
            
        except Exception as e:
            logger.error(f"ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹¤íŒ¨: {str(e)}")
    
    def get_usage_stats(self, days: int = 7) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ"""
        try:
            from datetime import timedelta
            
            total_cost = 0
            total_tokens = 0
            provider_stats = {}
            tier_stats = {}
            
            # ì§€ì •ëœ ì¼ìˆ˜ë§Œí¼ ì¡°íšŒ
            for i in range(days):
                date = (datetime.utcnow() - timedelta(days=i)).strftime('%Y-%m-%d')
                usage_key = f"ai_usage:{date}"
                daily_usage = self.redis_service.get_cache(usage_key) or []
                
                for usage in daily_usage:
                    total_cost += usage.get('cost', 0)
                    total_tokens += usage.get('tokens_used', 0)
                    
                    provider = usage.get('provider', 'unknown')
                    tier = usage.get('tier', 'unknown')
                    
                    provider_stats[provider] = provider_stats.get(provider, 0) + usage.get('cost', 0)
                    tier_stats[tier] = tier_stats.get(tier, 0) + usage.get('cost', 0)
            
            return {
                'period_days': days,
                'total_cost': round(total_cost, 4),
                'total_tokens': total_tokens,
                'avg_daily_cost': round(total_cost / days, 4),
                'cost_by_provider': provider_stats,
                'cost_by_tier': tier_stats,
                'cost_efficiency': 'excellent' if total_cost < 1.0 else 'good' if total_cost < 5.0 else 'review_needed'
            }
            
        except Exception as e:
            logger.error(f"ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {'error': str(e)}
    
    def get_available_models(self, tier: Optional[ModelTier] = None) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        models = []
        
        for model_config in self.models.values():
            if tier is None or model_config.tier == tier:
                models.append({
                    'name': model_config.name,
                    'provider': model_config.provider.value,
                    'tier': model_config.tier.value,
                    'cost_per_1k_tokens': model_config.cost_per_1k_tokens,
                    'max_tokens': model_config.max_tokens,
                    'context_window': model_config.context_window,
                    'strengths': model_config.strengths,
                    'best_for': model_config.best_for
                })
        
        return sorted(models, key=lambda x: x['cost_per_1k_tokens'])

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ai_provider_manager = AIProviderManager()

def get_ai_provider_manager() -> AIProviderManager:
    """AI ì œê³µì ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ai_provider_manager

# ëª¨ì˜ ëª¨ë“œ ì§€ì›
def get_llm_provider(use_mock: bool = None):
    """LLM ì œê³µì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹¤ì œ ë˜ëŠ” ëª¨ì˜)"""
    # í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ì˜ ëª¨ë“œ ê²°ì • (ê¸°ë³¸ê°’: ì‹¤ì œ ëª¨ë“œ)
    if use_mock is None:
        use_mock = os.getenv("USE_MOCK_AI", "false").lower() in ("1", "true", "yes")

    if use_mock:
        try:
            from app.services.mock_ai_provider import get_mock_ai_provider
            print("ğŸ­ ëª¨ì˜ AI ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤ (êµìœ¡ìš© ê³ í’ˆì§ˆ ì‘ë‹µ ì œê³µ)")
            return get_mock_ai_provider()
        except ImportError:
            print("âš ï¸ ëª¨ì˜ AI ì œê³µìë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ AI ì œê³µìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return ai_provider_manager

    print("ğŸš€ ì‹¤ì œ OpenRouter AI ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤")
    return ai_provider_manager

async def generate_ai_response(
    prompt: str,
    task_type: str = "general",
    model_preference: Optional[ModelTier] = None,
    user_id: Optional[int] = None,
    max_tokens: Optional[int] = None,
    temperature: float = 0.7
) -> Dict[str, Any]:
    """AI ì‘ë‹µ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    
    request = AIRequest(
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model_preference=model_preference or ModelTier.FREE,
        task_type=task_type,
        user_id=user_id
    )
    
    return await ai_provider_manager.generate_completion(request)

async def generate_feedback(
    user_answer: str,
    correct_answer: str,
    question_context: str,
    user_id: Optional[int] = None,
    use_premium: bool = False
) -> str:
    """í•™ìŠµ í”¼ë“œë°± ìƒì„± (íŠ¹í™” í•¨ìˆ˜)"""
    
    prompt = f"""ë‹¤ìŒ í•™ìŠµìì˜ ë‹µì•ˆì— ëŒ€í•´ ê±´ì„¤ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë¬¸ì œ ë§¥ë½: {question_context}

ì •ë‹µ: {correct_answer}

í•™ìŠµì ë‹µì•ˆ: {user_answer}

í”¼ë“œë°± ì§€ì¹¨:
1. ê¸ì •ì ì¸ ë¶€ë¶„ì„ ë¨¼ì € ì–¸ê¸‰
2. ê°œì„ í•  ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
3. í•™ìŠµì ìˆ˜ì¤€ì— ë§ëŠ” ì¶”ê°€ í•™ìŠµ ë°©í–¥ ì œì‹œ
4. ê²©ë ¤ì™€ ë™ê¸°ë¶€ì—¬ í¬í•¨

í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    tier = ModelTier.BASIC if use_premium else ModelTier.FREE
    
    response = await generate_ai_response(
        prompt=prompt,
        task_type="feedback",
        model_preference=tier,
        user_id=user_id,
        temperature=0.7
    )
    
    return response.get('response', 'í”¼ë“œë°± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')

async def analyze_learning_pattern(
    user_submissions: List[Dict],
    user_id: int,
    use_premium: bool = False
) -> Dict[str, Any]:
    """í•™ìŠµ íŒ¨í„´ ë¶„ì„ (íŠ¹í™” í•¨ìˆ˜)"""
    
    # ì œì¶œ ë°ì´í„° ìš”ì•½
    submissions_summary = []
    for submission in user_submissions[-10:]:  # ìµœê·¼ 10ê°œ
        submissions_summary.append({
            'topic': submission.get('topic', ''),
            'correct': submission.get('is_correct', False),
            'response_time': submission.get('response_time', 0),
            'difficulty': submission.get('difficulty', 1)
        })
    
    prompt = f"""ë‹¤ìŒ í•™ìŠµìì˜ ìµœê·¼ í•™ìŠµ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•™ìŠµ íŒ¨í„´ê³¼ ê°œì„  ë°©í–¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

í•™ìŠµ ë°ì´í„°: {json.dumps(submissions_summary, ensure_ascii=False, indent=2)}

ë¶„ì„í•´ì•¼ í•  ìš”ì†Œ:
1. ê°•ì  ì˜ì—­ê³¼ ì•½ì  ì˜ì—­ ì‹ë³„
2. í•™ìŠµ ì†ë„ì™€ ë‚œì´ë„ ì„ í˜¸ë„ ë¶„ì„
3. ê°œì„ ì´ í•„ìš”í•œ í•™ìŠµ ìŠµê´€
4. ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„ ì¶”ì²œ
5. ê°œì¸í™”ëœ í•™ìŠµ ì „ëµ ì œì•ˆ

ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
  "strengths": ["ê°•ì 1", "ê°•ì 2"],
  "weaknesses": ["ì•½ì 1", "ì•½ì 2"],
  "learning_speed": "fast/normal/slow",
  "preferred_difficulty": 1-5,
  "recommendations": ["ì¶”ì²œ1", "ì¶”ì²œ2"],
  "next_focus_areas": ["ì˜ì—­1", "ì˜ì—­2"]
}}"""

    tier = ModelTier.BASIC if use_premium else ModelTier.FREE
    
    response = await generate_ai_response(
        prompt=prompt,
        task_type="analysis",
        model_preference=tier,
        user_id=user_id,
        temperature=0.3
    )
    
    try:
        # JSON íŒŒì‹± ì‹œë„
        result = json.loads(response.get('response', '{}'))
        result['analysis_model'] = response.get('model', 'unknown')
        result['analysis_cost'] = response.get('cost_estimate', 0)
        return result
    except:
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            'strengths': ['ë¶„ì„ ë°ì´í„° ë¶€ì¡±'],
            'weaknesses': ['ë” ë§ì€ í•™ìŠµ í•„ìš”'],
            'learning_speed': 'normal',
            'preferred_difficulty': 3,
            'recommendations': ['ê¾¸ì¤€í•œ í•™ìŠµ ì§€ì†'],
            'next_focus_areas': ['ê¸°ì´ˆ ê°œë… ê°•í™”'],
            'raw_response': response.get('response', ''),
            'analysis_error': 'JSON íŒŒì‹± ì‹¤íŒ¨'
        }
