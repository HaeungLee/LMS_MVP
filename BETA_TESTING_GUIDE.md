# 🧪 LMS 베타 테스트 가이드

## 📋 개요

LMS MVP 베타 테스트에 오신 것을 환영합니다! 이 가이드는 베타 테스터, 개발팀, 그리고 관리자를 위한 포괄적인 안내서입니다.

## 🎯 베타 테스트 목표

### 주요 목표
1. **AI 기능 검증**: 실제 사용 환경에서 AI 기능들의 성능과 정확성 확인
2. **사용자 경험 개선**: 실제 사용자 피드백을 통한 UX/UI 개선점 발견
3. **시스템 안정성 확인**: 동시 사용자 환경에서의 시스템 안정성 검증
4. **성능 최적화**: 실제 워크로드에서의 성능 병목점 식별 및 개선

### 베타 기간
- **시작일**: 2024년 8월 25일
- **종료일**: 2024년 9월 25일 (1개월)
- **참여 인원**: 20명

## 👥 베타 테스터 가이드

### 🚀 시작하기

#### 1. 시스템 접속
- **웹 애플리케이션**: http://localhost:80
- **API 문서**: http://localhost:8000/docs

#### 2. 온보딩 프로세스
1. 베타 테스터 전용 온보딩 페이지 접속
2. 기본 정보 입력 (이름, 이메일, 학습 목표)
3. 베타 테스트 약관 동의
4. 초기 설정 완료

#### 3. 주요 기능 탐색
- **🧠 AI 심층 학습 분석**: 개인 학습 패턴 분석
- **🤖 AI 멘토링**: 24/7 AI 멘토와의 대화
- **⚡ 적응형 난이도**: 실시간 난이도 조정
- **🎯 개인화 학습 경로**: AI 기반 맞춤 커리큘럼
- **💻 AI 코드 리뷰**: 자동화된 코드 피드백
- **📊 프로젝트 분석**: AI 기반 프로젝트 평가

### 📝 피드백 제공 방법

#### 1. 즉시 피드백
- 각 기능 사용 후 나타나는 피드백 팝업 활용
- 버그 발견 시 즉시 신고 버튼 클릭

#### 2. 주간 피드백 설문
- 매주 금요일 발송되는 설문조사 참여
- 상세한 사용 경험 및 개선 제안 작성

#### 3. 심층 인터뷰 (선택사항)
- 월 2회 개별 인터뷰 참여 기회
- 30분 화상 인터뷰로 상세한 피드백 제공

### 🎯 테스트 시나리오

#### 주간 1: 기본 기능 탐색
- [ ] 계정 생성 및 프로필 설정
- [ ] 기본 학습 모듈 2개 이상 완료
- [ ] AI 멘토링 기능 최소 3회 사용
- [ ] 개인화 추천 기능 확인

#### 주간 2: AI 기능 집중 테스트
- [ ] AI 심층 분석 결과 확인 및 피드백
- [ ] 적응형 난이도 기능 체험
- [ ] AI 코드 리뷰 기능 테스트 (코드 제출)
- [ ] 개인화 학습 경로 생성 및 따라가기

#### 주간 3: 고급 기능 및 통합 테스트
- [ ] 프로젝트 생성 및 AI 분석 받기
- [ ] 포트폴리오 기능 활용
- [ ] 다양한 멘토 유형 테스트
- [ ] 장시간 사용 시나리오 테스트

#### 주간 4: 스트레스 테스트 및 최종 평가
- [ ] 동시 다중 기능 사용
- [ ] 대용량 데이터 처리 테스트
- [ ] 전체 시스템 종합 평가
- [ ] 최종 피드백 및 개선 제안

### 📊 성과 지표

베타 테스터로서 다음 지표들이 추적됩니다:
- **학습 진도**: 완료한 모듈 수 및 소요 시간
- **AI 기능 사용률**: 각 AI 기능별 사용 빈도
- **만족도 점수**: 주간 설문을 통한 만족도 평가
- **버그 신고**: 발견한 버그 수 및 품질
- **피드백 품질**: 제공한 피드백의 유용성

## 🛠️ 개발팀 가이드

### 📈 모니터링 및 관리

#### 1. 실시간 모니터링
- **시스템 대시보드**: http://localhost:3000 (Grafana)
- **메트릭 수집**: http://localhost:9090 (Prometheus)
- **베타 대시보드**: http://localhost:80/beta-dashboard

#### 2. 주요 모니터링 지표
- **시스템 성능**: CPU, 메모리, 디스크 사용률
- **API 성능**: 응답 시간, 처리량, 오류율
- **AI 기능 성능**: AI API 응답 시간, 성공률
- **사용자 활동**: 활성 사용자 수, 기능별 사용량

#### 3. 알림 설정
- **긴급**: 시스템 다운, 높은 오류율 (즉시 알림)
- **경고**: 성능 저하, AI API 오류 (30분 내 알림)
- **정보**: 사용량 급증, 새로운 피드백 (일일 요약)

### 🔧 배포 및 관리

#### 1. 배포 프로세스
```bash
# 자동 배포 스크립트 실행
python scripts/beta_deployment.py prod

# 모니터링 설정
python scripts/monitoring_setup.py

# 수동 배포 (필요시)
./deploy.sh prod
```

#### 2. 백업 및 복구
- **자동 백업**: 매일 02:00 AM 데이터베이스 백업
- **수동 백업**: 주요 업데이트 전 백업 생성
- **복구 절차**: 문제 발생시 이전 백업으로 롤백

#### 3. 로그 관리
```bash
# 애플리케이션 로그 확인
docker-compose logs -f backend

# AI 기능 로그 확인
docker-compose exec backend tail -f /app/logs/ai_features.log

# 베타 테스트 로그 확인
docker-compose exec backend tail -f /app/logs/beta_testing.log
```

### 📊 데이터 수집 및 분석

#### 1. 수집 데이터 유형
- **사용자 행동 데이터**: 클릭, 페이지 뷰, 세션 시간
- **기능 사용 데이터**: AI 기능별 사용 빈도 및 성공률
- **성능 데이터**: 응답 시간, 에러율, 리소스 사용량
- **피드백 데이터**: 정성적/정량적 사용자 피드백

#### 2. 데이터 분석 도구
- **Grafana**: 실시간 메트릭 시각화
- **Jupyter Notebook**: 데이터 분석 및 인사이트 도출
- **PostgreSQL**: 데이터 웨어하우스 및 분석 쿼리

#### 3. 주간 분석 리포트
- **사용자 활동 요약**: 주간 활성 사용자, 신규 가입자
- **기능별 성과**: AI 기능별 사용률 및 만족도
- **시스템 성능**: 평균 응답시간, 가용성, 오류율
- **피드백 요약**: 주요 피드백 및 개선 요청사항

## 📋 관리자 가이드

### 🎛️ 시스템 관리

#### 1. 사용자 관리
```bash
# 베타 테스터 목록 조회
curl -X GET "http://localhost:8000/api/v1/beta/users"

# 베타 테스터 추가
curl -X POST "http://localhost:8000/api/v1/beta/users" \
  -H "Content-Type: application/json" \
  -d '{"email": "tester@example.com", "role": "beta_tester"}'

# 사용자 활동 조회
curl -X GET "http://localhost:8000/api/v1/beta/metrics/usage/{user_id}"
```

#### 2. 설정 관리
- **베타 테스트 기간 설정**: 환경 변수 `BETA_TEST_END_DATE`
- **참가자 수 제한**: 환경 변수 `BETA_USER_LIMIT`
- **AI 기능 활성화**: 기능별 on/off 설정

#### 3. 비상 대응 절차
1. **즉시 대응** (시스템 다운)
   - 알림 확인 및 원인 파악
   - 필요시 이전 버전으로 롤백
   - 베타 테스터들에게 상황 안내

2. **정기 대응** (성능 이슈)
   - 성능 메트릭 분석
   - 병목점 식별 및 최적화
   - 개선 결과 모니터링

### 📈 성과 관리

#### 1. 주요 KPI
- **사용자 참여도**: DAU/MAU, 세션 시간, 재방문율
- **기능 채택률**: 신규 기능 사용률, 기능별 만족도
- **시스템 품질**: 가용성, 성능, 오류율
- **사용자 만족도**: NPS, 피드백 점수, 이탈률

#### 2. 위험 관리
- **기술적 위험**: 시스템 장애, 데이터 손실, 보안 이슈
- **사용자 위험**: 낮은 참여도, 부정적 피드백, 이탈
- **비즈니스 위험**: 일정 지연, 예산 초과, 목표 미달성

#### 3. 의사결정 지원
- **일일 리포트**: 핵심 지표 요약
- **주간 대시보드**: 트렌드 분석 및 이슈 식별
- **월간 평가**: 목표 대비 성과 평가 및 개선 계획

## 🚨 문제 해결

### 일반적인 문제들

#### 1. 로그인/접속 문제
- **증상**: 로그인 실패, 페이지 로딩 안됨
- **해결방법**: 
  - 브라우저 캐시 클리어
  - 다른 브라우저로 시도
  - VPN 해제 후 재시도

#### 2. AI 기능 작동 안함
- **증상**: AI 응답 없음, 오류 메시지
- **해결방법**:
  - 페이지 새로고침
  - 잠시 후 재시도 (API 제한일 수 있음)
  - 관리자에게 문의

#### 3. 성능 저하
- **증상**: 느린 응답, 타임아웃
- **해결방법**:
  - 네트워크 연결 확인
  - 다른 기능 먼저 사용
  - 피크 시간 외 사용 권장

### 🆘 지원 요청

#### 1. 버그 신고
- **방법**: 앱 내 버그 신고 버튼 또는 이메일
- **포함 정보**: 
  - 발생 시간
  - 수행한 동작
  - 오류 메시지 (있는 경우)
  - 스크린샷

#### 2. 기능 요청
- **방법**: 월간 피드백 세션 또는 이메일
- **포함 정보**:
  - 요청 기능 설명
  - 사용 사례
  - 우선순위

#### 3. 기술 지원
- **응답 시간**: 
  - 긴급: 1시간 이내
  - 일반: 24시간 이내
  - 기능 요청: 1주일 이내

## 📅 베타 테스트 일정

### Week 1: 기본 기능 검증
- 온보딩 프로세스 테스트
- 기본 학습 기능 사용
- 초기 피드백 수집

### Week 2: AI 기능 집중 테스트
- AI 멘토링 시스템 테스트
- 적응형 난이도 조정 검증
- 개인화 추천 정확도 평가

### Week 3: 통합 기능 테스트
- 전체 학습 플로우 테스트
- 프로젝트 관리 기능 검증
- 성능 및 안정성 테스트

### Week 4: 최종 검증 및 정리
- 종합 시나리오 테스트
- 최종 사용자 인터뷰
- 베타 종료 및 결과 분석

## 🎉 베타 완료 후

### 1. 결과 분석
- 수집된 데이터 종합 분석
- 사용자 피드백 정리 및 분류
- 개선 우선순위 결정

### 2. 제품 개선
- 버그 수정 및 성능 최적화
- 사용자 요청 기능 개발
- UI/UX 개선 사항 적용

### 3. 정식 출시 준비
- 베타 피드백 반영된 최종 버전 개발
- 운영 환경 최적화
- 마케팅 및 런칭 전략 수립

---

## 📞 연락처

- **기술 지원**: dev-support@lms-mvp.com
- **피드백**: beta-feedback@lms-mvp.com
- **긴급 문의**: emergency@lms-mvp.com
- **프로젝트 매니저**: pm@lms-mvp.com

---

*베타 테스트에 참여해 주셔서 감사합니다! 여러분의 소중한 피드백이 더 나은 LMS를 만드는 데 큰 도움이 됩니다.* 🙏
