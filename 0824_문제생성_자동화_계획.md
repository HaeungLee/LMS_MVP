# 🤖 AI 문제 생성 자동화 세부 계획 (0824)

## 🎯 현재 상황 분석

### ✅ **기존 문제 은행 시스템**
- **현재 방식**: 고정된 코드 스니펫 + 빈칸 채우기
- **문제 유형**: 주로 `fill_in_the_blank` 방식
- **한계점**: 
  - 문제 다양성 부족
  - 학습자 수준별 맞춤화 제한
  - 실시간 생성보다는 템플릿 기반

### 🚀 **AI 문제 생성 엔진 현황**
- ✅ OpenRouter API 연동 완료 (`google/gemma-2-9b-it`)
- ✅ curriculum_manager와 연동된 진도 기반 생성
- ✅ 8개 API 엔드포인트 구축 완료
- ✅ 동적 난이도 조정 시스템

---

## 📚 문제 유형 확장 계획

### 1️⃣ **주관식 문제 (Short Answer)**
```python
# 예시 생성 문제
{
    "type": "short_answer",
    "question": "파이썬에서 리스트와 튜플의 주요 차이점 3가지를 설명하세요.",
    "expected_keywords": ["변경 가능", "mutable", "순서", "성능"],
    "scoring_criteria": {
        "keyword_match": 0.4,
        "semantic_similarity": 0.6
    },
    "sample_answer": "리스트는 변경 가능하고 튜플은 불변입니다..."
}
```

### 2️⃣ **객관식 문제 (Multiple Choice)**
```python
# 예시 생성 문제
{
    "type": "multiple_choice",
    "question": "다음 중 파이썬 딕셔너리의 올바른 사용법은?",
    "options": [
        "A) dict[1, 2] = 'value'",
        "B) dict['key'] = 'value'", 
        "C) dict(1: 'value')",
        "D) dict['key', 'value']"
    ],
    "correct_answer": "B",
    "explanation": "딕셔너리는 key-value 쌍으로 데이터를 저장하며..."
}
```

### 3️⃣ **코드 완성 문제 (Code Completion)**
```python
# 기존 fill_in_the_blank 개선
{
    "type": "code_completion",
    "question": "다음 함수를 완성하여 리스트의 최댓값을 반환하세요.",
    "code_template": """
def find_max(numbers):
    max_val = numbers[0]
    for num in numbers:
        if ____:
            ____
    return ____
    """,
    "blanks": ["num > max_val", "max_val = num", "max_val"],
    "test_cases": [
        {"input": [1,3,2], "output": 3},
        {"input": [-1,-5,-2], "output": -1}
    ]
}
```

### 4️⃣ **디버깅 문제 (Debug Code)**
```python
{
    "type": "debug_code",
    "question": "다음 코드의 오류를 찾아 수정하세요.",
    "buggy_code": """
def calculate_average(numbers):
    total = 0
    for i in range(numbers):
        total += numbers[i]
    return total / len(numbers)
    """,
    "errors": [
        {"line": 3, "error": "range(numbers) should be range(len(numbers))"}
    ],
    "corrected_code": "수정된 코드..."
}
```

### 5️⃣ **실습 프로젝트 문제 (Project Task)**
```python
{
    "type": "project_task",
    "question": "간단한 계산기 프로그램을 작성하세요.",
    "requirements": [
        "덧셈, 뺄셈, 곱셈, 나눗셈 기능",
        "사용자 입력 처리",
        "0으로 나누기 예외 처리"
    ],
    "skeleton_code": "기본 구조 제공",
    "evaluation_criteria": [
        "기능 완성도", "코드 품질", "예외 처리"
    ]
}
```

---

## 🔧 구현 계획

### **Phase 1: 문제 유형 다양화 (2시간)**

#### 1-1. AI 프롬프트 템플릿 확장
```python
# ai_question_generator.py에 추가
QUESTION_TYPE_PROMPTS = {
    "multiple_choice": """
    주제: {topic}
    난이도: {difficulty}
    
    다음 조건으로 객관식 문제 1개를 생성하세요:
    - 4개의 선택지 (A, B, C, D)
    - 명확한 정답과 오답
    - 학습자가 개념을 이해했는지 확인하는 문제
    - 해설 포함
    
    JSON 형식으로 응답하세요.
    """,
    
    "short_answer": """
    주제: {topic}
    난이도: {difficulty}
    
    다음 조건으로 주관식 문제 1개를 생성하세요:
    - 2-3문장으로 답할 수 있는 개념 설명 문제
    - 핵심 키워드 3-5개 포함
    - 모범 답안 예시 포함
    
    JSON 형식으로 응답하세요.
    """
}
```

#### 1-2. 문제 생성 함수 확장
```python
async def generate_question_by_type(
    self, 
    question_type: str,
    topic: str, 
    difficulty: str,
    context: Dict = None
) -> Dict[str, Any]:
    """문제 유형별 생성 함수"""
```

#### 1-3. 채점 시스템 확장
```python
# scoring_service.py에 추가
async def score_by_question_type(
    self,
    question_type: str,
    question: Dict,
    user_answer: str,
    context: Dict = None
) -> Dict[str, Any]:
    """문제 유형별 채점 로직"""
```

### **Phase 2: 스마트 문제 생성 (3시간)**

#### 2-1. 학습자 맞춤형 생성
```python
async def generate_adaptive_questions(
    self,
    user_id: int,
    topic: str,
    count: int = 5
) -> List[Dict]:
    """
    학습자의 약점과 학습 패턴을 분석하여
    맞춤형 문제 생성
    """
    # 1. 사용자 학습 이력 분석
    weaknesses = await self.analyze_user_weaknesses(user_id, topic)
    
    # 2. 문제 유형 분배 결정
    question_mix = self.determine_question_mix(weaknesses)
    
    # 3. 각 유형별 문제 생성
    questions = []
    for q_type, count in question_mix.items():
        batch = await self.generate_questions_by_type(q_type, topic, count)
        questions.extend(batch)
    
    return questions
```

#### 2-2. 문제 품질 검증 시스템
```python
class QuestionQualityValidator:
    """생성된 문제의 품질을 자동 검증"""
    
    async def validate_question(self, question: Dict) -> Dict:
        """
        - 문법 정확성 검사
        - 난이도 적정성 확인  
        - 학습 목표 연관성 검증
        - 중복 문제 체크
        """
```

#### 2-3. 실시간 문제 추천 엔진
```python
class QuestionRecommendationEngine:
    """실시간으로 다음 문제를 추천"""
    
    async def recommend_next_question(
        self, 
        user_id: int,
        current_session: Dict,
        performance_history: List[Dict]
    ) -> Dict:
        """
        현재 세션 성과를 바탕으로
        다음 문제 유형과 난이도 결정
        """
```

### **Phase 3: 고급 기능 (2시간)**

#### 3-1. 문제 은행 하이브리드 시스템
```python
class HybridQuestionBank:
    """AI 생성 + 기존 문제 은행 통합"""
    
    async def get_optimal_question_set(
        self,
        requirements: Dict
    ) -> List[Dict]:
        """
        - 기존 검증된 문제 우선 활용
        - 부족한 부분만 AI로 실시간 생성
        - 품질과 효율성의 균형
        """
```

#### 3-2. 문제 생성 API 고도화
```python
# 새로운 엔드포인트 추가
@router.post("/generate-mixed-questions")
async def generate_mixed_question_set(
    request: QuestionGenerationRequest,
    current_user: User = Depends(get_current_user)
):
    """
    다양한 문제 유형이 혼합된 문제 세트 생성
    - 학습자 수준 분석
    - 최적 문제 유형 조합 결정
    - 실시간 생성 + 품질 검증
    """
```

#### 3-3. 교사용 문제 생성 도구
```python
# 교사가 문제 생성 설정을 세밀하게 조정
@router.post("/teacher/custom-question-generator")
async def teacher_custom_generator(
    settings: TeacherQuestionSettings,
    current_user: User = Depends(get_current_user)
):
    """
    교사용 고급 문제 생성 도구
    - 문제 유형 비율 조정
    - 특정 개념 집중 생성
    - 학급 전체 성향 반영
    """
```

---

## 🎯 우선순위 및 타임라인

### **즉시 구현 (오늘 완료 목표)**
1. ✅ **다양한 문제 유형 템플릿 추가** (30분)
2. ✅ **객관식/주관식 생성 함수 구현** (60분)  
3. ✅ **문제 유형별 채점 로직 추가** (30분)

### **Phase 1 완료 후 테스트**
```powershell
# 객관식 문제 생성 테스트
$mcRequest = '{"topic": "변수와 자료형", "question_type": "multiple_choice", "difficulty": "easy", "count": 2}'
$mcResult = Invoke-RestMethod -Uri "http://127.0.0.1:8000/api/v1/ai-learning/generate-questions" -Method POST -Body $mcRequest -ContentType "application/json" -WebSession $session

# 주관식 문제 생성 테스트  
$saRequest = '{"topic": "리스트와 딕셔너리", "question_type": "short_answer", "difficulty": "medium", "count": 1}'
$saResult = Invoke-RestMethod -Uri "http://127.0.0.1:8000/api/v1/ai-learning/generate-questions" -Method POST -Body $saRequest -ContentType "application/json" -WebSession $session
```

### **단계별 확장**
- **내일**: 스마트 문제 생성 + 품질 검증
- **모레**: 실시간 추천 엔진 + 교사용 도구
- **다음주**: 멀티미디어 문제 + 고급 분석

---

## 🔧 기술적 고려사항

### **1. AI 모델 최적화**
- **현재**: `google/gemma-2-9b-it` (범용 모델)
- **검토**: 코딩 특화 모델 `codellama/CodeLlama-7b-Instruct-hf`
- **하이브리드**: 문제 유형별 모델 선택

### **2. 성능 관리**
- **캐싱**: 자주 요청되는 주제/난이도 조합 사전 생성
- **배치 처리**: 여러 문제 동시 생성으로 효율성 극대화
- **폴백**: AI 실패시 기존 문제 은행 활용

### **3. 품질 보장**
- **A/B 테스트**: AI 생성 vs 기존 문제 학습 효과 비교
- **피드백 루프**: 교사/학생 평가를 통한 지속적 개선
- **검증 체계**: 자동 검증 + 인간 검토

---

## 📊 성공 지표

### **정량적 지표**
- 문제 생성 속도: 평균 10초 이내
- 문제 품질 점수: 교사 평가 4.0/5.0 이상
- 학습 효과: 기존 대비 학습 성취도 20% 향상
- 문제 다양성: 5가지 유형 × 3단계 난이도 = 15가지 조합

### **정성적 지표**  
- 학습자 만족도: "문제가 재미있고 도움이 된다"
- 교사 편의성: "문제 출제 시간이 크게 단축되었다"
- 시스템 안정성: "AI 생성 실패가 거의 없다"

---

## 🚀 다음 액션

**지금 바로 시작할 항목:**
1. **ai_question_generator.py** 확장 - 문제 유형별 템플릿 추가
2. **scoring_service.py** 확장 - 유형별 채점 로직
3. **API 엔드포인트** 수정 - question_type 파라미터 추가

**어떤 문제 유형부터 구현하시겠나요?**
- A) 객관식 (Multiple Choice) - 가장 구현하기 쉬움
- B) 주관식 (Short Answer) - 의미적 분석 필요
- C) 코드 완성 (Code Completion) - 기존 시스템 확장
- D) 전체 동시 구현 - 가장 완성도 높음

**결정해주시면 바로 구현 시작하겠습니다!** 🎯
